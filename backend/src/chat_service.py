"""èŠå¤©æœåŠ¡"""
import httpx
import os
import json
from typing import List, Dict, Any
from .config import settings
from .models import Message
from openai import OpenAI
from  fastapi.responses import StreamingResponse

class ChatService:
    """èŠå¤©æœåŠ¡ç±»"""
    
    def __init__(self):
        self.api_key = settings.deepseek_api_key
        self.base_url = settings.deepseek_base_url
        
        self.client = OpenAI(api_key=settings.deepseek_api_key,
                             base_url=settings.deepseek_base_url
                             )

    
    async def generate_response(
        self,
        messages: List[Message],
        mode: str = "Ask",
        model: str = "deepseek-chat",
        **kwargs
    ) -> str:
        """
        ç”ŸæˆAIå“åº”çš„ä¸»å…¥å£æ–¹æ³•
        
        æ ¹æ®ä¸åŒçš„æ¨¡å¼å’Œé…ç½®ï¼Œé€‰æ‹©åˆé€‚çš„å“åº”ç”Ÿæˆç­–ç•¥ï¼š
        - Askæ¨¡å¼: æ™®é€šå¯¹è¯å“åº”
        - Agentæ¨¡å¼: ä»£ç ç”Ÿæˆagentå“åº”
        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯å†å²åˆ—è¡¨
            mode: å¯¹è¯æ¨¡å¼ï¼ˆAsk/Agentï¼‰
            model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            **kwargs: å…¶ä»–é…ç½®å‚æ•°
            
        Returns:
            str: ç”Ÿæˆçš„AIå“åº”å†…å®¹
        """
        # æ£€æŸ¥APIå¯†é’¥é…ç½®
        if not self.api_key or self.api_key == "test-api-key":
            # è¿”å›æ¨¡æ‹Ÿå“åº”ç”¨äºæ¼”ç¤ºå’Œæµ‹è¯•
            user_message = messages[-1].content if messages else ""
            return self._generate_mock_response(user_message, model)
        
        try:
            # æ ¹æ®ä¸åŒæ¨¡å¼é€‰æ‹©å“åº”ç­–ç•¥
            if mode == "Ask":
                # æ™®é€šå¯¹è¯æ¨¡å¼
                model_answer = self._get_nonstreaming_response(messages, mode, model)
            elif mode == "Agent":
                # ä»£ç ç”Ÿæˆagentæ¨¡å¼
                model_answer = self._code_agent_llm_generate_response(
                    messages=messages, 
                    model=model, 
                    **kwargs
                )
            else:
                # æœªçŸ¥æ¨¡å¼ï¼Œé»˜è®¤ä½¿ç”¨Askæ¨¡å¼
                model_answer = self._get_nonstreaming_response(messages, "Ask", model)
            
            return model_answer
            
        except Exception as e:
            # ç”Ÿæˆå“åº”æ—¶çš„å¼‚å¸¸å¤„ç†
            error_message = f"ç”ŸæˆAIå“åº”æ—¶å‡ºé”™: {str(e)}"
            print(error_message)
            return error_message
    
    
    def _code_agent_llm_generate_response(self,
                                          messages: List[Message],
                                          model: str = "deepseek-reasoner",
                                          stream_callback = None,
                                          **kwargs):
        '''
            è‡ªå·±å†™ä»£ç çš„ai agent, ç®—æ³•ï¼š ReAct
        '''
        from .ai_code_agent.agent import get_code_agent_response
        
        # æ£€æŸ¥æ¨¡å‹
        if model not in settings.supported_LLM:
            return f"Current system can not support model {model}!"
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if "code_generation_root_dir" in kwargs:
            tar_dir = kwargs["code_generation_root_dir"]
        else:
            return ("in Function _code_agent_llm_generate_response: \
                kwargs has no code_generation_root_dir variable, the backend can not refer to correct code generation base_dir!!!!")
        
        os.makedirs(tar_dir, exist_ok=True)
        print(f"åˆ›å»ºäº†ç‹¬ç«‹ä»£ç ç›®å½•ï¼ï¼ï¼\n")
        
        messages[-1].content += f"ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ç¨‹åºå‘˜ï¼Œè¯·åœ¨æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„ï¼š{tar_dir} è¿›è¡Œä»£ç ç¼–å†™\n è¦æ±‚ï¼š\
            1.æ‰€æœ‰çš„æ“ä½œéƒ½åœ¨ä¸Šé¢çš„è·¯å¾„ä¸‹è¿›è¡Œï¼Œä¸èƒ½ä¿®æ”¹è·¯å¾„å¤–çš„ä»»ä½•ä¸œè¥¿ 2.ä»£ç ç®€ä»‹è§„èŒƒæœ‰æ³¨é‡Š"
        
        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºä»»åŠ¡
        task = messages[-1].content
        model_answer : str = get_code_agent_response(task, tar_dir, model, stream_callback)
        
        return model_answer

    def _code_agent_llm_generate_streaming_response(self,
                                                   messages: List[Message],
                                                   model: str = "deepseek-reasoner",
                                                   stream_callback = None,
                                                   **kwargs):
        '''
            æµå¼ç”Ÿæˆä»£ç çš„ai agent, ç®—æ³•ï¼š ReAct
        '''
        from .ai_code_agent.agent import get_code_agent_response
        
        # æ£€æŸ¥æ¨¡å‹
        if model not in settings.supported_LLM:
            if stream_callback:
                stream_callback(f"âŒ **é”™è¯¯**: å½“å‰ç³»ç»Ÿä¸æ”¯æŒæ¨¡å‹ {model}!")
            return f"Current system can not support model {model}!"
        
        # æ£€æŸ¥æ–‡ä»¶è·¯å¾„
        if "code_generation_root_dir" in kwargs:
            tar_dir = kwargs["code_generation_root_dir"]
        else:
            error_msg = "ä»£ç ç”Ÿæˆè·¯å¾„æœªæŒ‡å®šï¼Œæ— æ³•ç»§ç»­æ‰§è¡Œ"
            if stream_callback:
                stream_callback(f"âŒ **é”™è¯¯**: {error_msg}")
            return error_msg
        
        os.makedirs(tar_dir, exist_ok=True)
        print(f"åˆ›å»ºäº†ç‹¬ç«‹ä»£ç ç›®å½•ï¼ï¼ï¼")
        if stream_callback:
            stream_callback(f"ğŸ“ åˆ›å»ºä¸´æ—¶ä»£ç ç›®å½•ï¼ï¼ï¼ï¼\n")
        
        # æå–ç”¨æˆ·çš„æœ€åä¸€æ¡æ¶ˆæ¯ä½œä¸ºä»»åŠ¡
        task = ""
        for message in messages:
            task += f"{message.role}: {message.content}\n\n"
        task += f"**ä½ æ˜¯ä¸€ä¸ªç»éªŒä¸°å¯Œçš„ç¨‹åºå‘˜ï¼Œè¯·åœ¨æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„ï¼š{tar_dir} è¿›è¡Œä»£ç ç¼–å†™ è¦æ±‚ï¼š\
            1.æ‰€æœ‰çš„æ“ä½œéƒ½åœ¨ä¸Šé¢çš„è·¯å¾„ä¸‹è¿›è¡Œï¼Œä¸èƒ½ä¿®æ”¹è·¯å¾„å¤–çš„ä»»ä½•ä¸œè¥¿ 2.ä»£ç ç®€ä»‹è§„èŒƒæœ‰æ³¨é‡Š**"
        model_answer : str = get_code_agent_response(task, tar_dir, model, stream_callback)
        
        return model_answer

    # éæµå¼ä¼ è¾“æ•°æ®
    def _get_nonstreaming_response(
        self, 
        messages: List[Message], 
        mode: str = "Ask", 
        model: str = "deepseek-chat"
    ) -> str:
        """
        ç”Ÿæˆå®Œæ•´çš„AIå“åº”ï¼Œé€šè¿‡è°ƒç”¨DeepSeekçš„éæµå¼æ¥å£        
        Args:
            messages: å¯¹è¯æ¶ˆæ¯å†å²åˆ—è¡¨
            mode: å¯¹è¯æ¨¡å¼ï¼ˆAsk/Agentç­‰ï¼‰
            model: ä½¿ç”¨çš„LLMæ¨¡å‹åç§°
            
        Returns:
            str: å®Œæ•´çš„AIå“åº”å†…å®¹
        """
        #  ä¼šè¯é‡Œé¢æ·»åŠ messageï¼Œç„¶åä¸æ–­åœ°å¾€é‡Œé¢å¡«å……
    
    
        # å°†åç«¯messageæ ¼å¼è½¬åŒ–ä¸ºæ¨¡å‹éœ€è¦çš„æ ¼å¼
        formatted_messages = []
        for message in messages:
            formatted_messages.append({
                "role": message.role,
                "content": message.content
            })
        try:
            if model == "deepseek-chat":
                response = self.client.chat.completions.create(
                            model=settings.deepseek_chat_model,
                            messages=formatted_messages,
                            stream=False
                    )
            else:
                raise ValueError(f"Unsupported model: {model}")

            return response.choices[0].message.content
    
        except httpx.HTTPError as e:
            # å¤„ç†HTTPè¯·æ±‚å¼‚å¸¸
            error_message = f"HTTPè¯·æ±‚é”™è¯¯: {str(e)}"
            print(error_message)
            return error_message
        except Exception as e:
            # å¤„ç†å…¶ä»–å¼‚å¸¸
            error_message = f"ç”ŸæˆAIå“åº”æ—¶å‡ºé”™: {str(e)}"
            print(error_message)
            return error_message
    

# å…¨å±€èŠå¤©æœåŠ¡å®ä¾‹
chat_service = ChatService()
